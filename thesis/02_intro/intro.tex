%1. 31. Mai Einleitung, Motivation & Ziele, Strukturskizze der Arbeit
%(grob). => 4 Seiten
%Zusätzlich Skizze für DesignKapitel mit Stichpunkten (d.h. weiter
%ausführen von Kapitel 3., evtl. erste Bilder)

\section{Einleitung}
%%% Problematik %%%
% Problematik: Geschichtliche Entwicklung
In den letzten Jahren nahm die Leistungsfähigkeit der Hardware von Computersystemen zu: es erfolgte ein exponentielles Kapazitätswachstum des Massenspeichers, begleitet von einer vergleichsweise langsamen Zunahme der Datenübertragungsgeschwindigkeiten, die Integrierung mehrerer Kerne in die Prozessoren und einen explosionsartigen Wachstum von Datenmengen.

Früher war die Erhöhung der Taktfrequenz die Hauptmethode die Prozessorleistung zu steigern. 
Nachdem aber die physikalischen Grenzen der Halbleiterkomponenten erreicht wurden und die Erhöhung der Taktfrequenz nicht mehr möglich war, ist man dazu übergegangen Prozessoren mit mehreren Kernen zu auszustatten. 
Der Trend hält bis heute an und heute sind in meisten modernen Rechner Multikern-Prozessoren verbaut.

Die Fortschritte in der Massenspeicherentwicklung der letzten Jahre haben vor allem zur höheren Datendichte geführt, was die Herstellung von Geräten mit hoher Kapazität ermöglicht hat. 
Das Kapazitätswachstum, wie im Moore'schen Gesetz bereits formuliert, hat sich etwa alle anderthalb bis zwei Jahre verdoppelt bei gleichzeitig sinkendem Preis pro Speichereinheit.

Laut dem Bericht von IDC-Studie \cite{idc} verdoppelt sich auch das weltweite Datenvolumen alle zwei Jahre. 
Im Jahr 2013 wurde der Gesamtdatenvolumen auf etwas 4.4 Zetabyte geschätzt. 
Im Jahr 2020 soll dieser Wert auf 44 Zetabyte steigen. Die Datenproduktion wir von steigender Zahl der Geräte, wie digitale Kameras, mehr Leute produzieren Daten, es gibt immer mehr datenbasierte Dienste, wie Online-Data-Clouds.

% Problematik: Nutzung von IT-Systemen
In der Wirtschaft gibt es zwei große Bereiche, die auf hohe E/A-Leistung angewiesen sind. 
Das sind OLAP (Online Analytical Procesing) und OLTP (Online Transaction Processing). 
Im OLAP-Bereich nutzt man relativ große Datenbestände für die Informationsgewinnung. 
Die gewonnen Informationen sollen den Führungskräften und Managern bei den Entscheidungen helfen und deshalb liegt hier der Fokus auf den kurzen Reaktionszeiten. 
Im OLAP-Bereich hingegen geht es um die Datenspeicherung. 
Das Ziel ist die Daten im konsistenten Zustand zu behalten und die Transaktionenzahl zu maximieren.
Diese beiden Bereiche sind wahrscheinlich von dem Problem am schlimmsten betroffen.

Es gibt zahlreiche weitere Bereiche, die nicht weniger stark von dem Problem betroffen sind, z. B. die Verarbeitung der Messdaten in der Klimaforschung, Datensicherung im Unternehmen, interaktiven Knoten, auch private Computer.

\subsection{Problematik}

Die E/A-Leistung von HPC-Systemen wird hauptsächlich durch die Massenspeichergeschwindigkeit, die Kapazität der Datenübertragungskanäle und durch den Grad der Ausnutzung der Hardwarepotenzials beschränkt.
Das alles sind Bereiche, die eng miteinander zusammenhängen.
%Die zwei Probleme hängen direkt miteinander zusammen.

Die relativ langsamen Massenspeicher und Datenübertragungskanäle, im Vergleich zu Rechenleistung, sind das Resultat der unausbalancierter Hardware-Entwicklung in den letzten Jahren. 
Bei der Verarbeitung von genügend großen Datenmengen kann es deshalb zu Engpässen kommen, die das HPC-System ausbremsen können und stellen deswegen ein Problem dar.
Um dem entgegen zu wirken werden diverse Lösungen verwendet, wie Raid-Verbunde, parallele Dateisysteme, Caches und schnelle Hardware-Komponenten.
In HPC-Systemen werden meistens mehrere solche Lösungen gleichzeitig miteinander betrieben.
Das beschleunigt zwar die E/A-Leistung des gesamten Systems, dadurch entsteht aber auch ein komplexes Zusammenspiel zwischen den Komponenten, das es schwierig macht die E/A-Vorgänge zu überblicken und zu verstehen. 

Komplexe Systeme stellen auch besondere Ansprüche an die Software-Entwicklung, z. B. die Parallelisierung von Programmen und ein E/A-Zugriffsmanagement zum Erhalten der Datenkonsistenz.
Die Software wird dadurch oft größer und komplizierter und das E/A-Verhalten undurchschaubar.
Eine unzureichende Analyse vom Programmverhalten führt dann oft zur ineffizienten Hardwarenutzung, suboptimaler Kommunikation zwischen Rechen- und Speichereinheit und einer unausbalancierten Lastverteilung.
Je nach Komplexität, kann die Verhaltensanalyse für einen Menschen zu einer unüberwindlichen Hürde werden.
Ohne die genaue Vorstellung über die Vorgänge ist es kaum möglich das Programmverhalten vorherzusagen.
Schwierig ist es auch die Auswirkung von einer Programmänderung auf die Leistungsänderung (quantitativ) zu erfassen und noch schwieriger ist es die problematischen Programmteile zu identifizieren.

Der Administrationsbereich ist ebenfalls von dem Problem betroffen. 
Die Abstimmung der Hardware- und Softwarekomponenten aufeinander und auf die Optimierung auf die bevorstehende Nutzung kann zu schwierig sein. 
Die Nutzung kann dabei sehr unterschiedlich sein, z.B. kann man die Systeme auf kurze Reaktionszeiten oder großen Durchsatz optimiert.
Die Optimierung auf kurze Reaktionszeiten kommt vor allem für die interaktiven Systeme in Frage, wo viele Benutzer durch viele gleichzeitige E/A-Zugriffe, z. B. durch Lesen von einer Vielzahl von kleinen Dateien oder durch Ausführen von E/A-lastige Prozessen, schnell die Grenzen der Hardware ausreizen. 
Das ist wichtig, weil höhere Reaktionszeiten von Benutzern in der Regel als besonders störend empfunden werden. 
Bei Computern, die E/A-lastige Hintergrunddienste ausführen, macht sich das Problem durch geringen Systemauslastung bemerkbar. 

Die Problemursache kann in einem Satz zusammengefasst werden: die Prozessoren verbringen eine beträchtliche Zeit im Leerlauf, während sie auf die Daten warten. 

%\todo{1) Hardware ist langsam, wächst langsam, 2) Benutzung oft ineffizient, deswegen nur 10\% Leistung, z.b. Random I/O etc., tuning z.b. mit Hints, aber welche Hints sollen gesetzt werden 3) Analyse und Optimierung von E/A ist wichtig, wie geht das z.B. Vampir, Darshan und SIOX, Problem: Bewertung von Leistung, SIOX versucht das, aber ist sehr rudimentär, basierend auf modellen. Umständlich.}

% Problematik: E/A-Flaschenhals 
%So sind die Massenspeichermedien und deren Anbindung oft die langsameren Komponenten in einem Computer und bilden unter bestimmten Bedingungen einen Flaschenhals, der sich negativ auf die Antwortzeiten, Datendurchsatz und Auslastung des Systems aufwirkt. 

\subsection{Überblick über die Hardwarelösungen}
%%% Techniken um die E/A-Leistung zu verbessern %%%
Die Systemhersteller, Systembetreiber und die Wissenschaftler bemühen sich die Probleme mit verschiedenen Techniken in den Griff zu bekommen. 

% Raid
Viele Systeme, insbesondere Server und HPC-Systeme, sind mit Raid-Verbunden ausgestattet, wobei jedes Raid-Typ ein oder mehrere Ziele verfolgen kann. 
Die meisten Raid-Verbunde versuchen jedoch ein Kompromiss zwischen Leistung, Datensicherheit und Kosten zu finden, wobei die Ziele gegeneinander gerichtet sind, was bedeutet, dass man das eine nicht verbessern kann ohne das andere zu verschlechtern. 
Die relativ hohen Anschaffungs-, Betriebs- und Wartungskosten machen die Raid-Systeme nur bis zur einer gewissen Größe wirtschaftlich. 
Außerdem erreichen die Raid-Verbunde eine höhere Leistung nur für Daten ab einer bestimmten Größe. 
Der Grund liegt in der Funktionsweise der Raid-Verbunde. 
Sie zerteilen die Daten in \todo{evtl. hier weiter Ausbauen,\\oder vielleicht besser im nächsten Kapitel} Blöcke und schreiben diese sie parallel auf mehrere Massenspeicher. 
Wenn die Daten nicht auf mehrere Blöcke zerteilen lassen, dann kann keine paralleler Schreibzugriff stattfinden, deshalb auch kein Leistungsgewinn. 
Für den Lesezugriff gilt die analoge Logik.

%\todo{Parallele Dateisysteme auch hier erwähnen, RAID ist ein älterer Hut}
% Parallele verteilte Dateisysteme
Verteilte Dateisystem fassen mehrere entfernte Massenspeicher zu einem Dateisystem zusammen. 
Für den Benutzer bleibt die verteilte Struktur Dateisystems nicht sichtbar und er kann darauf wie auf lokales Dateisystem zugreifen. 
Somit erfüllen die verteilte Dateisysteme das Zugriffstransparenzkriterium. 
Die Kommunikation mit dem Dateisystem läuft über eine fest definierte Schnittstelle ab und macht die verteilten Dateisysteme betriebssystem- und hardwareunabhängig. 
Eine Besonderheit von vielen modernen verteilten Dateisystemen ist ihre Fähigkeit parallel auf die Daten zuzugreifen. 
Sie dabei können verschiedene Strategien verwenden. 
Ähnlich wie die Raid-Verbunde, können sie die Dateien in mehrere Blöcke zerteilen und parallel auf mehrere Massenspeicher verteilen. 
Bekannte Vertreter dieser Dateisystemfamilie in HPC-Bereich sind Lustre, GPFS. \todo{Relevanz von Cache}

% Cache
Die Caches eine weitere weitverbreitete Methode die E/A-Leistung zu verbessern. 
Dabei werden die oft genutzte oder voraussichtlich bald benötigte Daten auf einem schnelleren Datenträger zwischenzuspeichert, z.B. ist in den meisten modernen Internet-Browsern das HTTP-Caching implementiert. 
Die Inhalte der Webseiten werden auf der Festplatte abgelegt, um beim wiederholten Zugriff bereits besuchte Seiten schnell aufzubauen und unnötigen Datenverkehr zu vermeiden. 
OLTP-Datenbankenbetreiber (On-Line-Transaction Processing), bauen zusätzlich zu Festplattenverbunden SSD-Zwischenspeicher in Ihre Systeme ein, um die Transaktionenanzahl zu erhöhen. 
OLAP-Lösungen setzen vermehrt auf die In-Memory Analyse um Zugriffszeiten auf E/A zu verzichten.
Das Dateisystem ZFS bittet die Möglichkeit Arbeitsspeicher als Cache zu nutzen. \todo{ZFS erläutern}

% Hardware
\todo{Würde Hardware oben erzählen :}%, NVRAM http://en.wikipedia.org/wiki/Non-volatile_memory http://de.wikipedia.org/wiki/Phase-change_random_access_memory}
Ein direkter Weg die E/A-Leistung zu verbessern ist die Verbesserung der Hardware und hier können wir wahrscheinlich die meisten Entwicklungen sehen. 
Als Beispiel kann man die Entwicklung von Massenspeicher anschauen. 
Während der gesamten Entwicklung wurden die Drehzahlen der Festplatten erhöht, die Schreib- / Lesekopfgeschwindigkeiten verbessert, die Datendichte, die Cachegröße und die unterschiedliche Zugriffsstrategien und Optimierung der Schreib- /Lesekopfwegen angewendet. 
Die letzten Generationen der Festplatten wurden mit Edelgas befüllt und haben Flüssigkeitsgleitlager. 
Einen wesentlichen Schritt hat man mit der Einführung der SSD gemacht. 

% Busse
\todo{IDE, SATA, SAS fehlt}

% Loesungsansaetze sind nicht ausreichend
Trotz allen Bemühungen verschärft sich das Problem zunehmend und stellt eine Herausforderung an die Forscher eine wirtschaftliche und leistungsfähige Lösung zu entwickeln. 
Zahlreiche Unternehmen und Forschungseinrichtungen haben das Problem bereits angegangen und zahlreiche Projekte gestartet, die sehr unterschiedliche Wege gehen.

% aktuelle Forschung
HP forscht an einem neuartigen Datenspeicher \cite{hp_memristor_future}, der mit Hilfe von Memristoren realisiert werden und den flüchtigen schnellen Arbeitsspeicher und den nichtflüchtigen langsamen Massenspeicher vereinheitlichen soll. 
Solch ein Speicher ermöglicht auch eine Computerarchitektur, in der die E/A-Problematik zumindest lokal wesentlich entschärft werden kann. 
Der erste Prototyp soll laut dem Zeitplan im Jahr 2017 erscheinen und im Jahr 2020 sollen die Rechner dem breiten Markt verfügbar sein. 


\subsection{Analyse und Optimierung von E/A-Operationen}
Um die meist kostspielige Erweiterung oder den Ersatz von Hardware zu vermeiden oder hinauszuzögern bietet sich als Alternative die Optimierung der Software.
Die Optimierung kann man auf verschiedenen Wegen erreichen. 

% Ausnutzung der Lokalität
Leistungssteigerung ist auch direkt bei der Programmierung möglich, z. B. kann man die Lokalität der Daten ausnutzen. 
Wenn man genau weiß wie die Daten im Speicher abgelegt werden, dann kann man sein E/A-Befehl auf kleine Anzahl von E/A-Zugriffen optimieren. 
Allerdings setzt dies ein tiefes Verständnis des Systems und sehr gute Kenntnisse in einer hardwarenahen Programmiersprache voraus. 
In der Praxis ist diese Methode nur auf einfache Fälle anwendbar. 
Außerdem garantiert es nicht, dass die Optimierungen auf anderen Rechner genauso effizient laufen. 
Fertige Programme kann man nicht optimieren.

% Analyse von CPU-Last

% Analyse von Kommunikation



Bei der Analyse und Optimierung der von E/A-Operationen muss man die Hardware mit in Betracht ziehen und die Besonderheiten beachten.
Schließlich will man letztendlich die Hardware optimal nutzen. 
% Systematische Herangehensweise and die Leistungsanalyse 
\todo{Ausbauen,\\SIOX papers, MPI hints, tuning von Dateisystemen,\\Problem mit der Analyse der E/A, Darshan und SIOX erwähnen...}
Eine andere interessante Forschungsrichtung ist die Optimierung der E/A-Aufrufe. 
In einem Zweig geht man davon aus, dass Systeme, die parametrisierten E/A-Aufrufe anbieten das Potenzial haben mit optimalen Parametern beschleunigt zu werden. 
Die Schwierigkeit dabei für jeden individuellen E/A-Aufruf die optimalen Parameter zu bestimmen und zu übergeben. 
Schafft man das, dann kann die Leistung in vielen Fällen zumindest theoretisch wesentlich verbessert werden. 
In einem anderen Zweig versucht man die E/A-Zugriffe so zuordnen, damit möglichst optimal auf die Hardware abgestimmt sind. 
Beides ist universell anwendbar, kann als Software implementiert werden und relativ kostengünstig dem breiten Publikum zur Verfügung gestellt werden.

Wie bereits angedeutet, kommen bei der Realisierung der E/A-Systeme verschiedene Hard- und Software-Technologien zum Einsatz, z.B beim Zugriff auf die Festplatten spielt das Dateisystem, Bus, Blockgröße, diverse Caches usw. eine Rolle. 
Alle Komponenten stehen in einem komplexen Zusammenhang miteinander und führen zu Leistungseinbußen bei einer suboptimalen Konfiguration. 
Die Abstimmung der Komponenten aufeinander alleine erfordert meistens schon Expertenwissen, hinzu kommt noch Abstimmung der Komponenten auf die Nutzung, die in manchen Fällen variieren und unvorhersagbar sein kann. 
Eine universelle Methode oder sogar eine Software, die diese Aufgabe erleichtert, wäre hier sehr willkommen. 
Dazu fehlt allerdings noch die Grundlage, weil Vorgänge auf der E/A-Ebene wurden bisher nicht ausreichend untersucht sind. 
Es fehlt Wissen über die Zugriffsmuster, die bei der komplexen Nutzung entstehen, deren Ursachen und Auswirkungen auf die Systemleistung. 
Dementsprechend fehlen auch Gegenmaßnahmen zur Vermeidung von unerwünschten Situationen.



\subsection{Ziele und Vorgehensweise}

\todo{Das ausbauen und klarer darstellen. Ein bis zwei Satze was du machst und was das bringt, dannach die Teilziele klar darstellen }
%Eine einfache und gleichzeitig effektive Methode könnte die Parameteroptimierung bei parametrisierbaren E/Augriffen sein. 
%Der Gedanke geht davon aus, dass die an die aktuelle Situation dynamisch angepasste Parameter bei manchen Systemen zur einer wesentlich besseren Leistung führen können, als die statisch festgelegten Werte. Das Ziel wäre dann solch eine Methode zu entwickeln und zu untersuchen.

% messen und verstehen
% Programm Verhalten
% Machine Learning
% Rule Extraction

%\subsection{Aufgabenstellung}
%%% Vorgehensweise: Potezial pruefen, Potenzial nutzen, Wissen ableiten %%%
% Werkzeuge: Machine Learning, SIOX

%\todo{Das ist noch etwas unkonkret}
%Diese Arbeit soll ein Blick hinter die Kulissen auf der E/A-Ebene werfen und einige Parameter identifizieren, die ein Einfluss auf die Systemleistung haben. Dann soll geprüft werden, inwieweit die Parameteroptimierung eine Leistungsverbesserung hervorrufen kann und das Potenzial bestimmt werden. Schließlich soll eine Methode zur systematischen Analyse und Optimierung mit Hilfe von Machine-Learning-Algorithmen entwickelt werden. 

Ziel der Arbeit ist die Analyse und Bewertung von Ein-/Ausgabe-Operationen mittels maschinellem Lernen vorzunehmen.
Hierfür wird das SIOX-Framework erweitert um das maschinelle Lernen zu ermöglichen und Bewertungs-Plugins mit verschiedenen Granularitäten erstellt.
Im Detail gliedern sich die Teilziele des Projektes in:

\begin{itemize}
	\item Workflow-Design und Umsetzung.
	\item Vorhersage von Durchsatz von E/A-Operationen.
	\item Bewertung der E/A-Operationen zur Laufzeit des Programms und Ausgabe einer Statistik nach der Terminierung. 
		In der Bewertung wird den E/A-Operationen ein Typ zugeordnet (z.B.  Zwischenspeicherzugriff, direkter Zugriff, \dots), die Statistik zeigt die Häufigkeit an.
	\item Wissensgewinn durch Extraktion von Regeln aus dem verwendeten Machine-Learning-Model.
\end{itemize}


\todo{Noch ein prickelnder Schlussatz}

%Das primäre Ziel ist es einen Performance-Predictor und einen Classificator zu bauen, die das Systemverhalten widerspiegeln und dann mit den beiden Tools ein Parametrisierungstool erstellen, der für den beabsichtigten E/A-Zugriff möglichst optimale Parameter liefert. Der Performance-Predictor und Classificator werden mit Hilfe von Machine-Learning realisiert.

%Das sekundäre Ziel ist es versuche Regeln aus den Predictor zu extrahieren, die das Systemverhalten beschreiben. Dazu wird die interne Struktur von bereits trainierten Machine-Learning-Modelle im Performance-Predictor untersucht und das Zustandekommen von Entscheidungen in einer kurzen und menschenleserlichen Form notiert.

%Zum Erfassen von E/A-Zugriffen wird SIOX verwendet werden, ein Tool fängt alle Systemaufrufe vom Programm ab und stellt sie für die Analyse zur Verfügung. Die Datenmenge soll auf Anomalien und Strukturen untersucht werden. Die Ursache für die Entstehung der Anomalien und die Strukturbildung soll erklärt werden.

\subsection{Aufbau der Arbeit}
% Verwandte Arbeiten: 6 Stueck
% Theorie:
	% Begriffe: aus Machine-Learning, zur Leistungsbeschreibung
	% Techniken: Arbeitsweise von Cache, Festplattenmechanik, Betriebsysteme
	% Datenfluss: Speicher -> Cache -> Verarbeitung, Leistungsverluste
	% Werkzeuge: SIOX, Machine-Learning
% Design
  % Predictor-Design
  % Classificator-Design
% Implementierung: Predictor, Classificator, Rule Extraction
% Auswertung: 
% Schlussworte
\todo{Am Ende nochmal überarbeiten}
\todo{Etwas detaillierter}
Das zweite Kapitel stellt einige verwandte Arbeiten aus dem Bereich der E/A-Zugriffsoptimierung vor. 
Im dritten Kapitel werden die theoretischen Aspekte behandelt. 
Dort werden die relevanten Begriffe definiert, die Funktionsweise der aktuellen Technologie erläutert und die benutzten Werkzeuge vorgestellt. 
Das vierten Kapitel beschreibt das Design des Predictors und des Classificators und erklärt wie man mit diesen beiden Werkzeugen ein Parametrisierungstool zum setzen von optimalen Parametern bauen kann.
Im fünften Kapitel wird die Umsetzung vorgestellt und die Ergebnisse ausgewertet. Zum Schluss gibt es noch eine Zusammenfassung und den Ausblick auf weitere Forschungsmöglichkeiten.
